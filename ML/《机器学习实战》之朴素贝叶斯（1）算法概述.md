# 《机器学习实战》之朴素贝叶斯（1）算法概述

----------

- **转载请注明作者和出处：http://blog.csdn.net/u011475210**
- **代码地址：https://github.com/WordZzzz/ML/tree/master/Ch04**
- **操作系统：WINDOWS 10**
- **软件版本：python-3.6.2-amd64**
- **编&emsp;&emsp;者：WordZzzz**

----------

[toc]

## 前言：

&emsp;&emsp;本周WordZzzz学习了朴素贝叶斯（naive Bayes）。朴素贝叶斯是基于贝叶斯定理与特征条件独立假设的分类方法，对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布，然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。朴素贝叶斯发实现简单，学习与预测的效率都很高，是一种常用的方法。

## 基于贝叶斯决策理论的分类方法

&emsp;&emsp;朴素贝叶斯：
- 优点：在数据较少的情况下仍然有效，可以处理多类别问题。
- 缺点：对于输入数据的准备方式较为敏感。
- 适用数据类型：标称型数据。

&emsp;&emsp;朴素贝叶斯是贝叶斯决策理论的一部分，所以在此之前我们有必要快速了解一下贝叶斯决策理论。

&emsp;&emsp;假设现在我们有一个数据集，它由两类数据组成，数据分布如下图所示。

<p></p>
<div align=center><img src="http://img.blog.csdn.net/20170909224840023?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMTQ3NTIxMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="600" height="400" /></div>
<p></p>

&emsp;&emsp;假设WordZzzz找到了描述图中两类数据的统计参数。现在用P1(x,y)表示数据点(x,y)属于类别1（图中用原点表示的类别）的概率，用P2(x,y)表示数据点(x,y)属于类别2（图中用三角形表示的类别）的概率，那么对于一个新数据点(x,y)，可以用下面的规则来判断它的类别：
- 如果P1(x,y) > P2(x,y)，那么类别为1.
- 如果P1(x,y) < P2(x,y)，那么类别为2.

&emsp;&emsp;也即是说，我们会选择该改路对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。

&emsp;&emsp;回到上图，如果图中的整个数据使用6个浮点数来表示，并且计算类别概率的Python代码只有两行，那么你会更加倾向于使用下面哪种方法来对该数据点进行分类？
- (1)使用第一章的kNN，进行1000次距离计算；
- (2)使用第二章的决策树，分别沿x轴、y轴划分数据；
- (3)计算数据点属于每个类别的概率，并进行比较。

&emsp;&emsp;使用决策树不会非常成功；而和简单的概率计算相比，kNN的计算量太大。因此，对于上述问题，最佳选择是使用刚才提到的概率比较方法。

&emsp;&emsp;这里使用的概率解释属于贝叶斯概率理论的范畴，该理论非常流行且效果良好，贝叶斯概率以18世纪的以为神学家Thomas Bayes的名字命名。贝叶斯概率引入先验知识和逻辑推理来处理不确定命题。另一种概率解释称为频数概率，他只从数据本身获得结论，并不考虑逻辑推理及先验知识。

## 条件概率

&emsp;&emsp;接下来讲讲概率和条件概率。还好，就一两个公式。

&emsp;&emsp;假设现在有一个装了7块石头的罐子，其中三快事灰色的，4块是黑色的，如下图所示。

<p></p>
<div align=center><img src="http://img.blog.csdn.net/20170909224900451?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMTQ3NTIxMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="400" height="200" /></div>
<p></p>

&emsp;&emsp;如果从罐子里随机抽取一块石头，那么是灰色石头的可能性是多少？很显然，灰色石头概率3/7，黑色石头概率4/7。我们使用P(gray)来表示取到灰色石头的概率，其概率值可以通过灰色石头数目除以总数目来得到。

&emsp;&emsp;如果这7块石头放在两个桶里，如下图所示，那么上述概率如何计算？

<p></p>
<div align=center><img src="http://img.blog.csdn.net/20170909224951714?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMTQ3NTIxMA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="800" height="300" /></div>
<p></p>

&emsp;&emsp;要计算P(gray)或者P(black)，事先得到石头所在桶的信息会不会改变结果？加定计算的是从B桶中取到灰色石头的概率，这个概率记作P(gray|bucketB)，我们称之为“在已知石头出自B桶的条件下，取出灰色石头的概率”。不难的到P(gray|bucketA)的值为2/4，P(gray|bucketB)的值为1/3。

&emsp;&emsp;条件概率的计算公式如下所示：

`!$P(gray|bucketB) = P(gray and bucketB)/P(bucketB)$`

&emsp;&emsp;首先，用B桶中灰色石头的个数除以两个桶中的总石头数，得到P(gray and bucketB)=1/7。其次，由于B桶中有三块石头，而总石头数为7，于是P(bucketB)就等于3/7。于是有P(gray|bucketB) = P(gray and bucketB)/P(bucketB) = (1/7)/(3/7) = 1/3。这个公式虽然对于简单例子来说有点复杂，因为像WordZzzz一样的大部分明眼人一眼就能看出结果而没必要使用公式，但是当存在更多特征时，这个公式是非常有效的。用代数方法计算条件概率时，该公式也很有用。

&emsp;&emsp;另一种有效计算条件概率的方法被称为贝叶斯准则。贝叶斯准则告诉我们如何交换条件概率中的条件和结果，即如果已知P(x|c)，要求P(x|c)，那么可以使用下面的计算方法：

`!$ P(c|x) = \frac {P(x|c)P(c)} {P(x)} $`

&emsp;&emsp;知道条件概率怎么算之后，接下来的问题就是如何将其应用到分类器中。接下来，我们将讨论如何结合贝叶斯决策理论使用条件概率。

## 使用条件概率来分类

&emsp;&emsp;贝叶斯决策理论要求计算两个概率P1(x,y)和P2(x,y)：

- [ ] 如果P1(x,y) > P2(x,y)，那么属于类别1；
- [ ] 如果P1(x,y) < P2(x,y)，那么属于类别2。

&emsp;&emsp;但这两个准则并不是贝叶斯决策理论的所有内容，使用P1()和P2()只是为了尽可能简化描述，而真正需要计算和比较的是`!$P(c_1|x,y)$`和`!$P(c_2|x,y)$`。这些符号所代表的具体意义是：给定某个由x、y表示的数据点，那么该数据点来自类别`!$c_1$`的概率是多少？数据点来自类别`!$c_2$`的概率又是多少？注意这些概率与刚才给出的概率`!$P(x,y|c_1)$`并不一样，不过可以使用贝叶斯准则来交换概率中条件与结果。具体地，应用贝叶斯准则得到：

`!$ P(c_i|x,y) = \frac {P(x,y|c_i)P(c_i)} {P(x,y)} $`

&emsp;&emsp;使用这些定义，可以定义贝叶斯分类准则为：

- [ ] 如果`!$P1(c_i|x,y) > P2(c_i|x,y)$`，那么属于类别1；
- [ ] 如果`!$P1(c_i|x,y) < P2(c_i|x,y)$`，那么属于类别2。

&emsp;&emsp;使用贝叶斯准则，可以通过已知的三个概率值来计算未知的概率值。后面就会给出利用贝叶斯准则来计算概率并对数据进行分类的代码。

## 使用朴素贝叶斯进行文档分类

&emsp;&emsp;机器学习的一个重要应用就是文档的自动分类。在文档分类中，整个文档是实例，而电子邮件中的某些元素则构成特征。虽然电子邮件是一种会不断增加的文本，但我们同样可以对新闻报道、用户留言、政府公文等其他任意类型的文本进行分类。朴素贝叶斯是贝叶斯分类器的一个扩展，是用于文档分类的常用算法。

朴素贝叶斯的一般流程：
- 收集数据：可以使用任何方法，本章使用RSS源。
- 准备数据：准备数值型或者布尔型数据。
- 分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好。
- 训练算法：计算不同的独立特征的条件概率。
- 测试算法：计算错误率。
- 使用算法：一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本。

&emsp;&emsp;由统计学知，如果每个特征需要N个样本，那么对于10个特征将需要`!$N^{10}$`个样本，对于包含1000个特征的词汇表将需要`!$N^{1000}$`个样本。吓我一跳，所欲要的样本数会随着特征数目增大而迅速增长。

&emsp;&emsp;如果特征之间相互独立，那么样本数就可以减少到1000*N，所谓独立指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系，但是我们知道这种假设并不正确。这个假设正式朴素贝叶斯分类器中朴素（naive，天真！）一词的含义。朴素贝叶斯分类器中的另一个假设是，每个特征同等重要。其实这个假设也有问题。如果要判断留言是否得当，那么可能不需要看完所有的1000个单词，而只需要看10-20个就够了。尽管上述假设存在一些瑕疵，但朴素贝叶斯的实际效果却很好。

**<font color="red" size=3 face="仿宋">系列教程持续发布中，欢迎订阅、关注、收藏、评论、点赞哦～～(￣▽￣～)～</font>**

**<font color="red" size=3 face="仿宋">完的汪(∪｡∪)｡｡｡zzz</font>**